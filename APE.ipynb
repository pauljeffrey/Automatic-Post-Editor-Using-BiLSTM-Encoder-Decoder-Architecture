{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"APE.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMF0emqBhoujcY/OuePM+O9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"LTL8q3-dZd1E","executionInfo":{"status":"ok","timestamp":1626608513376,"user_tz":-60,"elapsed":1121,"user":{"displayName":"jeffrey otoibhi","photoUrl":"","userId":"11067368294353522262"}}},"source":["import tensorflow as tf\n","from tensorflow.keras.layers import Bidirectional\n","from tensorflow.keras.layers import Layer\n","import keras\n","from keras import layers\n","from tensorflow.keras.callbacks import LearningRateScheduler\n","\n","from tensorflow.keras.layers.experimental import preprocessing\n","from sklearn.feature_extraction.text import TfidfVectorizer as Vectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","import numpy as np\n","\n","from nltk.translate.bleu_score import sentence_bleu\n","from nltk.translate.bleu_score import SmoothingFunction\n","import warnings\n","warnings.filterwarnings('ignore')\n"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"5SuW1cLrbTLw"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D24xDsZUZZZm","executionInfo":{"status":"ok","timestamp":1626608130863,"user_tz":-60,"elapsed":8,"user":{"displayName":"jeffrey otoibhi","photoUrl":"","userId":"11067368294353522262"}}},"source":["# First, we define the attention class. It was implemented from the Bahdanau et al 2014 research paper on attention mechanism.\n","\n","class BahdanauAttention(tf.keras.Model):\n","  def __init__(self, units):\n","    super(BahdanauAttention, self).__init__()\n","    self.W1 = tf.keras.layers.Dense(units)\n","    self.W2 = tf.keras.layers.Dense(units)\n","    self.V = tf.keras.layers.Dense(1)\n","\n","   \n","\n","  def call(self, enc_hidden, dec_inter_hidden):\n","    # hidden here means 'hidden_size', word length the same as 'sentence length'\n","    # enc_hidden shape == (batch_size, word_length, hidden)\n","    # dec_inter_hidden shape == (batch_size, word_length, hidden)\n","    \n","    # attention_hidden_layer shape == (batch_size, word_length, units)\n","    #dec_inter_hidden = tf.expand_dims(dec_inter_hidden, 1)\n","    attention_hidden_layer = (tf.nn.tanh(self.W1(enc_hidden) +\n","                                         self.W2(dec_inter_hidden)))\n","\n","    # score shape == (batch_size, word_length, 1)\n","    # Score is a function of the intermediate hidden of the decoder and the encoder hidden\n","    # This gives you an unnormalized score for each word.\n","    score = self.V(attention_hidden_layer)\n","\n","    # attention_weights shape == (batch_size, word_length, 1)\n","    attention_weights = tf.nn.softmax(score, axis=1)\n","\n","    # context_vector shape after sum == (batch_size, hidden)\n","    context_vector = attention_weights * enc_hidden # (batch_size, word_length, hidden)\n","    context_vector = tf.reduce_sum(context_vector, axis=1) #(batch_size, hidden)\n","\n","    return context_vector, attention_weights"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"avtTwd49ZhUN","executionInfo":{"status":"ok","timestamp":1626608130865,"user_tz":-60,"elapsed":7,"user":{"displayName":"jeffrey otoibhi","photoUrl":"","userId":"11067368294353522262"}}},"source":["# Build encoder model\n","\n","class Encoder(tf.keras.Model):\n","  def __init__(self, units,embedding_dim,src_vocab_size, mt_vocab_size,dropout=0.2):\n","    super(Encoder, self).__init__()\n","    # Define bidirectional encoder for source language\n","    self.src_encoder = layers.Bidirectional(layers.GRU(units,\n","                                   return_sequences=True,\n","                                   return_state=True,\n","                                   recurrent_initializer='glorot_uniform',\n","                                   dropout= dropout))\n","    \n","    # Define bidirectional encoder for machine translated output language\n","    self.mt_encoder = layers.Bidirectional(layers.GRU(units,\n","                                   return_sequences=True,\n","                                   return_state=True,\n","                                   recurrent_initializer='glorot_uniform',\n","                                   dropout = dropout))\n","    # Embedding layer for both src and mt\n","    self.src_embed_layer = tf.keras.layers.Embedding(src_vocab_size, embedding_dim,mask_zero=True)\n","    self.mt_embed_layer = tf.keras.layers.Embedding(mt_vocab_size, embedding_dim, mask_zero=True)\n","\n","    # Embed dropout\n","    self.src_embed_dropout = tf.keras.layers.Dropout(dropout)\n","    self.mt_embed_dropout = tf.keras.layers.Dropout(dropout)\n","\n","    # Define dropout \n","    self.dropout = tf.keras.layers.Dropout(dropout)\n","    #self.embed_dropout  = tf.keras.layers.Dropout(dropout)\n","\n","  def call(self, src, mt):\n","    # Embed both src and mt words:\n","    src = self.src_embed_layer(src)\n","    mt = self.mt_embed_layer(mt)\n","\n","    # Add dropout\n","    src = self.src_embed_dropout(src)\n","    mt = self.mt_embed_dropout(mt)\n","\n","    # first, we encode both src and mt:\n","    src, src_state,_ = self.src_encoder(src)\n","    mt, mt_state,_ = self.mt_encoder(mt)\n","\n","    # Apply shared dropout to both src_encode and mt_encode\n","    src = self.dropout(src)\n","    mt = self.dropout(mt)\n","\n","    return src , mt, mt_state"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"OjW60IBjZlA1","executionInfo":{"status":"ok","timestamp":1626608239384,"user_tz":-60,"elapsed":453,"user":{"displayName":"jeffrey otoibhi","photoUrl":"","userId":"11067368294353522262"}}},"source":["# Build decoder model\n","class Decoder(tf.keras.Model):\n","  def __init__(self,units,att_units, linear_units, vocab_size,embedding_dim,dropout =0.2):\n","    # Initialize with parent class\n","    super(Decoder, self).__init__()\n","    self.units = units\n","    # embedding layer for target words\n","    self.embed  = tf.keras.layers.Embedding(vocab_size, embedding_dim,mask_zero=True)\n","\n","    # Decoder uses two GRUs to compute the probability of the next post-edited word\n","    self.gru1 = tf.keras.layers.GRU(units,\n","                                   return_sequences=True,\n","                                   return_state=True,\n","                                   recurrent_initializer='glorot_uniform')\n","    self.gru2 = tf.keras.layers.GRU(units,\n","                                   return_sequences=True,\n","                                   return_state=True,\n","                                   recurrent_initializer='glorot_uniform',\n","                                    dropout = dropout)\n","    \n","    #Define attention class for source sentence embedding:\n","    self.src_attention = BahdanauAttention(att_units)\n","    # Define attention class for machine translated sentence embedding:\n","    self.mt_attention = BahdanauAttention(att_units)\n","\n","    # Define linear transformation for final concatenated context.\n","    self.linear = tf.keras.layers.Dense(linear_units)    \n","    \n","    # Define embed dropout layer\n","    self.embed_dropout = tf.keras.layers.Dropout(dropout)\n","    # Define context dropout layer\n","    self.dropout = tf.keras.layers.Dropout(dropout)\n","\n","    # Define fully connected layer\n","    self.fc = tf.keras.layers.Dense(vocab_size)\n","\n","  \n","    \n","  def call(self, prev_word, hidden_state, src_encode ,mt_encode ):\n","    # This decoder model predicts the next word in 2 steps:\n","    # First, it uses the previous word embedding (prev_word_embed) and hidden state (hidden_state) to compute an intermediate state.\n","    # second, it uses the new intermediate hidden state (inter_hidden)and the context_vector to predict the next word in the target sentence (hpe)\n","\n","    # Get embedding of previous word , shape = (batch_size, 1, embedding_dim)\n","    prev_word = self.embed(prev_word)\n","\n","    # Add dropout to embed layer\n","    prev_word = self.embed_dropout(prev_word)\n","\n","    # Expand the hidden state by 1 more dimension (batch_size, 1, hidden)\n","    hidden_state = tf.expand_dims(hidden_state,1)\n","\n","    # we first merge the previous word embedding and hidden_state of the second gru\n","    #hidden_merge shape = (batch_size, 1, hidden + embedding_dim)\n","    hidden_merge  = tf.concat([tf.expand_dims(prev_word,1), hidden_state], -1)\n","\n","    # pass to the gru, inter_hidden shape = (batch_size, units)\n","    #hidden_merge = tf.expand_dims(hidden_merge,1)\n","    inter_hidden , state = self.gru1(hidden_merge) \n","    #inter_hidden  = tf.squeeze(inter_hidden)\n","    \n","    # Use attention mechanism to extract context from each encoded entities:\n","    src_context, _  = self.src_attention(src_encode, inter_hidden)\n","    mt_context, _  = self.mt_attention(mt_encode, inter_hidden)\n","\n","    #print(src_context.shape, mt_context.shape)\n","    # Merge both context and transform result linearly:\n","    # merged_context = tf.concat([src_context, mt_context],-1)\n","    context_vector = self.linear( tf.concat([src_context, mt_context],-1))\n","\n","    # Apply dropout to the context:\n","    context_vector = self.dropout(context_vector)\n","\n","    # concatenate intermediate state and context_vector then, pass to gru2\n","    # Expand context by# Define the loss class\n","\n","    context_vector = tf.expand_dims(context_vector, 1)\n","\n","    #hidden_context_merge = tf.concat([inter_hidden, context_vector], -1)\n","    #hidden_context_merge = tf.expand_dims(hidden_context_merge,1)\n","    final_hidden, state = self.gru2(tf.concat([inter_hidden, context_vector], -1))\n","    #print(final_hidden.shape, inter_hidden.shape)\n","    # Run through a fully connected layer to predict logits (before softmax) of next word:\n","    # output shape = (batch_size, vocab_size)\n","    output = self.fc(final_hidden)\n","\n","    # return output and hidden state of gru2\n","    return output, state\n"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"uPn_uoehZnnt","executionInfo":{"status":"ok","timestamp":1626608311421,"user_tz":-60,"elapsed":472,"user":{"displayName":"jeffrey otoibhi","photoUrl":"","userId":"11067368294353522262"}}},"source":["\n","class MaskedLoss(tf.keras.losses.Loss):\n","  def __init__(self):\n","    self.name = 'masked_loss'\n","\n","    self.loss = tf.keras.losses.SparseCategoricalCrossentropy(\n","        from_logits=True, reduction='none')\n","\n","  def __call__(self, y_true, y_pred):\n","    # Calculate the loss for each item in the batch.\n","    loss = self.loss(y_true, y_pred)\n","\n","    # Mask off the losses on the elements that were padded in the hpe sentences.\n","    mask = tf.cast(y_true != 0, tf.float32)\n","    loss *= mask\n","\n","    # Return the mean loss.\n","    return tf.reduce_mean(loss) #one more dimension first\n","\n","# Define full model\n","class APE(tf.keras.Model):\n","  def __init__(self,units,embedding_dim, att_units, linear_units,\n","               src_text_processor, mt_hpe_text_processor, dropout=0.2):\n","    # initialize with parent class:\n","    super(APE, self).__init__()\n","\n","    # initialize encoder:\n","    self.encoder  = Encoder(units,embedding_dim,src_text_processor.vocabulary_size(),\n","                            mt_hpe_text_processor.vocabulary_size(),dropout= dropout)\n","\n","    # Initialize decoder:\n","    self.decoder  = Decoder(units, att_units, linear_units, mt_hpe_text_processor.vocabulary_size(), \n","                            embedding_dim,dropout =dropout)\n","\n","    # Set the preprocessors for both source , machine translated and human post edited sentences:s\n","    self.src_processor = src_text_processor\n","    self.mt_hpe_processor = mt_hpe_text_processor\n","\n","  def _preprocess(self,src , mt , hpe):\n","    # This function preprocesses the source, mt and hpe sentences into respective tokens using the preprocessors.\n","    src_token = self.src_processor(src)\n","    mt_token = self.mt_hpe_processor(mt)\n","    hpe_token = self.mt_hpe_processor(hpe)\n","    return src_token, mt_token , hpe_token\n","\n","  # Define the customized training function:\n","  @tf.function\n","  def train_step(self, data):\n","    # Unpack the data.\n","    src , mt, hpe = data\n","\n","    # Preprocess source, mt and hpe sentences:\n","    src , mt , hpe = self._preprocess(src, mt , hpe)\n","    \n","    max_target_length = tf.shape(hpe)[1]\n","   \n","    # Set loss to 0.0\n","    loss = tf.constant(0.0)\n","\n","    # Set the first input to the decoder model to the '[START]' token which is the first element of each sentence:\n","    dec_input = hpe[:,0]\n","\n","    with tf.GradientTape() as tape:       \n","      # Pass the source and machine translated sentences to the encoder as inputs.\n","      # we use the returned hidden state of the encoder to initialize the decoder's hidden state:\n","      src_encode , mt_encode , dec_hidden = self.encoder(src, mt)\n","\n","      # Use for loop to iterate through each word as we predict the next word:\n","      for i in range(1,max_target_length):\n","      # Make predictions\n","        # Pass dec_input (serves as previous words), dec_hidden and the encoded src and mt to the decoder.\n","        predictions , dec_hidden = self.decoder(dec_input, dec_hidden, src_encode, mt_encode)\n","\n","        # Calculate the loss\n","        loss  = loss + self.loss(hpe[:,i],predictions)\n","\n","        # Using teacher forcing:\n","        # Teacher forcing continually feeds the next correct word in the real hpe (target) sentences to the model\n","        # instead of passing what the model predicted back into the model.\n","        dec_input = hpe[:, i]\n","\n","        \n","      \n","      # Average the loss over all non padding tokens.\n","      average_loss = loss / tf.cast(max_target_length, tf.float32)\n","\n","    # Apply an optimization step\n","    variables = self.trainable_variables \n","    gradients = tape.gradient(average_loss, variables)\n","    self.optimizer.apply_gradients(zip(gradients, variables))\n","\n","    # Return a dict mapping metric names to current value\n","    return {'batch_loss': average_loss}\n","    \n","\n","    "],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"9KOdM1wfaFcv","executionInfo":{"status":"ok","timestamp":1626608526093,"user_tz":-60,"elapsed":461,"user":{"displayName":"jeffrey otoibhi","photoUrl":"","userId":"11067368294353522262"}}},"source":["def lr_time_based_decay(epoch, lr):\n","    return lr * 1 / (1 + decay * epoch)\n","\n","\n","\n","class APETranslator(tf.Module):\n","  def __init__(self,\n","               encoder, decoder, \n","               src_text_processor,\n","               mt_hpe_text_processor):\n","    # Initialize encoder, decoder, src_processor, mt_hpe_processor:\n","    self.encoder = encoder\n","    self.decoder = decoder\n","    self.src_text_processor = src_text_processor\n","    self.mt_hpe_text_processor = mt_hpe_text_processor\n","\n","    # We initialize a string look up that will convert tokens to their equivalent words from the src and mt vocabulary:\n","    # We use the StringLookup class.\n","    self.output_token_string_from_index = (\n","        tf.keras.layers.experimental.preprocessing.StringLookup(\n","            vocabulary= mt_hpe_text_processor.get_vocabulary(),\n","            invert=True))\n","\n","    # The output should never generate padding, unknown, or start.\n","    # Therefore, get the index of '', '[UNK]' and '[START]' from the vocabulary:\n","    index_from_string = tf.keras.layers.experimental.preprocessing.StringLookup(\n","        vocabulary= mt_hpe_text_processor.get_vocabulary())\n","    token_mask_ids = index_from_string(['',\n","                                        '[UNK]',\n","                                        '[START]']).numpy()\n","\n","    # Create a token mask by first creating an array of tokens (vocabulary) and then initialize all to False\n","    # Set the indexes of the '', '[UNK]' and '[START]' to True:\n","    token_mask = np.zeros([index_from_string.vocabulary_size()], dtype=np.bool)\n","    token_mask[np.array(token_mask_ids)] = True\n","\n","    # set as a property of the class\n","    self.token_mask = token_mask\n","\n","    # Get the index of the '[START]' and '[END]' tokens:\n","    \n","    self.start_token = index_from_string('[START]')\n","    self.end_token = index_from_string('[END]')\n","\n","  def tokens_to_text(self, result_tokens):\n","    # This method converts index tokens to the string tokens by using the stringlookup we initialized in the init() method:\n","    result_text_tokens = self.output_token_string_from_index(result_tokens)\n","    \n","    # Join tokens to form a full string separating with a ' ':\n","    result_text = tf.strings.reduce_join(result_text_tokens,\n","                                        axis=1, separator=' ')\n","\n","    # Strip any extra ' ' from the text:\n","    result_text = tf.strings.strip(result_text)\n","   \n","    return result_text\n","\n","  def sample(self, logits, temperature):\n","\n","    # Add 2 new axis to the token_mask shape\n","    token_mask = self.token_mask[tf.newaxis, tf.newaxis, :]\n","\n","    # Set the logits for all masked tokens ('','[START]', '[UNK]') to -inf, so they are never chosen.\n","    # The logits (shape== (batch_size, vocab_size) here represents the independent probabilities of each word in the vocabulary.\n","    logits = tf.where(token_mask, -np.inf, logits)\n","    # If temperature is 0, get the max logit argument for each sample in the batch:\n","    # This argument represents the index of the word with the highest independent probability.\n","    if temperature == 0.0:\n","      new_tokens = tf.argmax(logits, axis=-1)\n","    # Else, sample from the independent probabilities of the logit:\n","    else: \n","      logits = tf.squeeze(logits, axis=1)\n","      new_tokens = tf.random.categorical(logits/temperature,\n","                                          num_samples=1)\n","    return new_tokens\n","\n","  def single_post_edit(self, src, mt, max_length=50, temperature=1.0):\n","    result = []\n","    src = self.src_text_processor(src)\n","    mt = self.mt_hpe_text_processor(mt)\n","    src, mt, dec_hidden= self.encoder(src, mt)\n","    dec_input = tf.fill((1,),self.start_token)\n","    #tf.expand_dims([tokenizer.word_index['<start>']], 0)\n","    \n","    for i in range(max_length):\n","        # TensorShape([1, 1, 15000]), TensorShape([1, 1024]))\n","        predictions, dec_hidden = self.decoder(dec_input, dec_hidden, src, mt)\n","\n","        predicted_id = tf.random.categorical(tf.squeeze(predictions,1)/temperature, 1)[0][0].numpy()\n","        #print(predicted_id)\n","        \n","        result.append(self.output_token_string_from_index(tf.cast(predicted_id, tf.int64)).numpy())\n","\n","        if predicted_id == self.end_token:\n","            return b' '.join(result[:-1])\n","\n","        dec_input = tf.fill((1,), predicted_id)\n","    \n","    return b' '.join(result)\n","\n","  #@tf.function\n","  def post_edit(self, src, mt, max_length=50,temperature=1.0):\n","\n","    batch_size = tf.shape(src)[0]\n","\n","    # Encode the input\n","    src_tokens = self.src_text_processor(src)\n","    mt_tokens = self.mt_hpe_text_processor(mt)\n","\n","    # Encode inputs:\n","    src_enc, mt_enc , dec_state = self.encoder(src_tokens, mt_tokens)\n","\n","    # Initialize the decoder\n","    new_tokens = tf.fill((batch_size,), self.start_token)\n","\n","\n","    # Initialize the accumulators for the decoder output:\n","    result_tokens = tf.TensorArray(tf.int64, size=1, dynamic_size=True)\n","    # Initialize a tf array that tracks the end_token of all samples in the batch:\n","    done = tf.zeros([batch_size, 1], dtype=tf.bool)\n","    #print(f'outside loop: {new_tokens.shape}', dec_state.shape)\n","\n","    for t in tf.range(max_length):\n","      # Pass through decoder model:\n","      #print('yes')\n","      #print('Beginning of loop',new_tokens.shape)\n","      dec_result, dec_state = self.decoder(new_tokens, dec_state, src_enc, mt_enc)\n","      #print('NO')\n","      new_tokens = self.sample(dec_result, temperature)\n","      #print('After self.sample',new_tokens.shape)\n","      # If a sequence produces an `end_token`, set it `done`\n","      done = done | (new_tokens == self.end_token)\n","      # Once a sequence is done it only produces 0-padding.\n","      new_tokens = tf.where(done, tf.constant(0, dtype=tf.int64), new_tokens)\n","\n","      # Collect the generated tokens\n","      result_tokens = result_tokens.write(t, new_tokens)\n","      new_tokens = tf.squeeze(new_tokens)\n","\n","      if tf.reduce_all(done):\n","        break\n","\n","    # Convert the list of generates token ids to a list of strings.\n","    result_tokens = result_tokens.stack()\n","    result_tokens = tf.squeeze(result_tokens, -1)\n","    result_tokens = tf.transpose(result_tokens, [1, 0])\n","\n","    result_text = self.tokens_to_text(result_tokens)\n","\n","    return {'text': result_text}\n","\n","# Define a function to always get an APE Translator object from a trained model ( useful in the online learning section):\n","def get_post_editor(model):\n","  return APETranslator(ape.encoder, ape.decoder, ape.src_processor, ape.mt_hpe_processor)\n","\n","\n","\n","def get_bleu_score(y_true, y_pred):\n","  # This function calculates the cummulative BLEU-4 score by default:\n","  #y_pred = y_pred.numpy()\n","  scores = []\n","  #smooth_func = SmoothingFunction()\n","  # Get human post edit and machine post edit from both y_true, y_pred\n","  for hpe, mpe in zip(y_true, y_pred):\n","    hpe = [hpe.split(b' ')]\n","    mpe = mpe.split(b' ')\n","    score = sentence_bleu(hpe,mpe)\n","    scores.append(score)\n","  return np.array(scores).mean()"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"eP802KOfbF7T","executionInfo":{"status":"ok","timestamp":1626608613516,"user_tz":-60,"elapsed":562,"user":{"displayName":"jeffrey otoibhi","photoUrl":"","userId":"11067368294353522262"}}},"source":["\n","def TER_score(y_true, y_pred):\n","  # This function calculates the cummulative TER score by default:\n","  #y_pred = y_pred.numpy()\n","  scores = []\n","  # Get human post edit and machine post edit from both y_true, y_pred\n","  for hpe, mpe in zip(y_true, y_pred):\n","    score = pyter.ter(mpe.split(),hpe.split())\n","    scores.append(score)\n","  return np.array(scores).mean()\n","\n","def get_blue_ter(y_true,y_pred):\n","  bleu_score = get_bleu_score(y_true, y_pred)\n","  ter_score = TER_score(y_true, y_pred)\n","\n","  print(f'BLEU Score: {bleu_score:.2f}')\n","  print(f'TER Score:  {ter_score:.2f}')"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"ikXIVbgebbLw","executionInfo":{"status":"ok","timestamp":1626608717698,"user_tz":-60,"elapsed":448,"user":{"displayName":"jeffrey otoibhi","photoUrl":"","userId":"11067368294353522262"}}},"source":["# Function removers the new line character from each sentence in the dataset:\n","def preprocess(text):\n","  text = [t.replace(b'\\n',b'') for t in text]\n","  text = [t.lower() for t in text]\n","  return text\n","\n","\n","# Define a sort_score function to sort the datapoints.\n","def sort_score(scores):\n","  result = []\n","  for index, score in enumerate(list(scores[0])):\n","    result.append((index,score))\n","  result.sort(key=lambda x: x[1],reverse=True)\n","  return result\n","\n"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"xFdnKbEEb0uy","executionInfo":{"status":"ok","timestamp":1626608732804,"user_tz":-60,"elapsed":450,"user":{"displayName":"jeffrey otoibhi","photoUrl":"","userId":"11067368294353522262"}}},"source":["# Initialize the learning rate for the model.\n","initial_learning_rate = 0.001\n","# We schedule a learning rate for every epoch using the formula below:\n","# For every epoch, learning rate is reduced using this formula.\n","\n","\n","# We use a time based decay to schedule the learning rate which will be appropriate for online learning.\n","def lr_time_based_decay2(epoch, lr):\n","    return lr * 1 / (1 +  epoch)\n","\n","\n","def set_hyperparameters(score, threshold,initial_learning_rate):\n","  # Get all values above threshold:\n","  score = score[score[:,1] >= threshold]\n","  # Get mean score:\n","  score = score[:,1]\n","  mean = score.mean()\n","  if mean >= 0.85:\n","    epochs= 8\n","  elif mean >= 0.65:\n","    epochs = 5\n","  else:\n","    epochs = 3\n","  return epochs \n","\n","\n","def model_compile(model,learning_rate):\n","  model.compile(\n","    optimizer=tf.optimizers.Adam(learning_rate=learning_rate),\n","    loss=MaskedLoss(),\n","    )\n","  return model\n","\n","def get_human_post_edit(src,ape):\n","  print(f'Source sentence: {src}')\n","  print(f'Machine post-edit translation: {ape}')\n","  print('')\n","  satisfied = input('Are you satisfied with the translation?: (Y for Yes and N for No) \\t')\n","\n","  if satisfied == 'Y':\n","    #print('yes')\n","    train = False\n","    #ape = ape.encode('utf-8')\n","    return [ape], train\n","  elif satisfied == 'N':\n","    #print('No')\n","    good_answer  =  input('Please provide yours below:\\n')\n","    while (good_answer == '' or good_answer == ' '):\n","      good_answer = input('Please provide a valid translation below:')\n","    good_answer  = good_answer.encode('utf-8')\n","    train = True\n","    return [good_answer], train\n","  else:\n","    #print('nothing')\n","    train = False\n","    return [''], train\n","  \n"," \n","def get_datastream(src, mt, hpe, batch_size=5):\n","  buffer_size = len(src)\n","  BATCH_SIZE = batch_size\n","\n","  # This Dataset class shuffles the data and precreates batches for it.\n","  dataset = tf.data.Dataset.from_tensor_slices((src, mt, hpe)).shuffle(buffer_size)\n","  dataset = dataset.batch(BATCH_SIZE)\n","  dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n","  return dataset\n","\n","def online_ape( sent_stream, ape_model, vectorizer,ckpt_manager=None, \n","               data=None, epochs= 3,threshold=0.35,initial_learning_rate=0.001, interactive=False):\n","  '''\n","    This function is implemented from Matteo Negri et al (2017) as explained above.\n","    Args:\n","    sent_stream: development dataset for online training (i.e src, mt ,hpe)\n","    ape_model: new or pretrained ape model. If new model, data cannot be empty.\n","    ckpt_manager: checkpoint manager.\n","    vectorizer: vectorizer object that vectorizes sentences.\n","    data: knowledge base (kb) that stores all the processed/learned sent_stream sets and/or ape training data in format (src, mt , hpe).\n","    epochs: Number of iterations to train the model on data.\n","    threshold: similarity score threshold to use when retrieving similar datapoints.\n","    interactive: Used to interact directly with any user who chooses to provide the right human post edit for every src, mt pair.\n","  '''\n","  # Unpack\n","  if not interactive:\n","    src, mt, hpe = sent_stream\n","    # Reverse list for popping:\n","    src.reverse()\n","    mt.reverse()\n","    hpe.reverse()\n","  else:\n","    src, mt = sent_stream\n","    # Reverse list for popping:\n","    src.reverse()\n","    mt.reverse()\n","\n","  # while sentence still remains in the stream, pop one sentence from the stream:\n","  while(len(src) != 0):\n","    # Get one sentence at a time:\n","    src_sent = [src.pop()] \n","    mt_sent = [mt.pop()] \n","    # Retrieve sentences similar to this sentence from the dataset:\n","    # If no dataset (knowldedge base):\n","    if not data:\n","      # Make copy of generic model and predict translation:\n","      #model = deepcopy(ape_model)\n","      # Create the translator version of this generic model:\n","      editor = get_post_editor(ape_model)\n","      \n","    else:\n","      # Vectorize source sentence:\n","      sentence = vectorizer.transform(src_sent)\n","      # Calculate the similarity between sentence and dataset source sentences:\n","      similarity_score = cosine_similarity(sentence, vectorizer.transform(np.array(data[0])))\n","      # sort similarity scores with respective index and convert to an np array:\n","      similarity_score = np.array(sort_score(similarity_score))\n","      # Get relevant index of sentences above threshold value:\n","      relevant_index = similarity_score[similarity_score[:,1] >= threshold]\n","\n","      # Get the relevant samples from the data using the index:\n","      relevant_src = np.array(data[0])[relevant_index[:,0].astype('int32')]\n","      relevant_mt = np.array(data[1])[relevant_index[:,0].astype('int32')]\n","      # ind = relevant_index[:,0].astype('int32')\n","      # print('Indexes', ind)\n","      # for i in range(len(ind)):\n","      #   print(np.array(data[2])[i])\n","      #print(np.array(data[2])[relevant_index[:,0].astype('int32')])\n","      relevant_hpe = np.array(data[2])[relevant_index[:,0].astype('int32')]\n","\n","      #relevant_src[0].split())\n","\n","      if relevant_src.shape[0] >= 3:\n","        #print('Sample size: ', relevant_src.shape[0])\n","        # Dynamically get number of epochs and learning rate. Then compile model using the learning_rate:\n","        epochs = set_hyperparameters(similarity_score, threshold, initial_learning_rate)\n","        #ape_model = model_compile(ape_model,learning_rate)\n","        \n","        # Locally adapt similar sentences to model:\n","        print(f'Training on {relevant_src.shape[0]} similar sample sentences...')\n","        # Get dataset object:\n","        dataset = get_datastream(list(relevant_src), list(relevant_mt), list(relevant_hpe))\n","        # Train model:\n","        ape_model.fit(dataset,epochs=epochs,\n","                      callbacks=[LearningRateScheduler(lr_time_based_decay)])\n","\n","      # Get translator version of the trained model:\n","      editor  = get_post_editor(ape_model)\n","\n","    # If dataset is empty, create the right structure for it:\n","    if not data:\n","      data = [[],[],[]]\n","\n","    if interactive :\n","      # Predict translation of the sentence:\n","      pe = editor.single_post_edit(src_sent,mt_sent)\n","      #pe = pe.encode('utf-8')\n","      #predictions.append(pe)\n","      hpe_sent , train = get_human_post_edit(src_sent[0], pe)\n","    else:\n","      train = True\n","      hpe_sent = [hpe.pop()]\n","\n","    # Update model according to new sentence if train is set to True\n","    if train:\n","      # Get dataset object:\n","      dataset = get_datastream(src_sent,mt_sent,hpe_sent, batch_size=1)\n","      # Train model:\n","      ape_model.fit(dataset, epochs=epochs, callbacks=[LearningRateScheduler(lr_time_based_decay2)])\n","    # Append the result to the dataset knowledge base (data):\n","    data[0].extend(src_sent)\n","    data[1].extend(mt_sent)\n","    if hpe_sent[0] == '':\n","      data[2].extend([pe])\n","    else:\n","      data[2].extend(hpe_sent)\n","    \n","\n","  if ckpt_manager:\n","    print('Online training completed. Saving model parameters...')\n","\n","    ckpt_manager.save()\n","\n","  return \n","\n","\n","\n","\n","  \n","  \n"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lz26olnZb4ZN"},"source":[""],"execution_count":null,"outputs":[]}]}